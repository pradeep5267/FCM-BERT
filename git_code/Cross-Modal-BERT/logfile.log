attention mask =====> torch.Size([24, 1, 1, 1])
attention mask squeeze =====> torch.Size([24, 1, 1])
attention mask permutation =====> torch.Size([24, 1, 1])
text data =====> torch.Size([24, 50, 768])
text data transpose =====> torch.Size([24, 768, 50])
text data projection =====> torch.Size([24, 30, 50])
text data transpose 1,2 =====> torch.Size([24, 50, 30])
torch.Size([24, 50, 5])
torch.Size([24, 5, 50])
torch.Size([24, 30, 50])
torch.Size([24, 50, 30])
end of audio data
torch.Size([24, 50, 30])
torch.Size([24, 30, 50])
end of text data
torch.Size([24, 50])


attention mask =====> torch.Size([24, 1, 1, 1])
attention mask squeeze =====> torch.Size([24, 1, 1])
attention mask permutation =====> torch.Size([24, 1, 1])
text data =====> torch.Size([24, 50, 768])
text data transpose =====> torch.Size([24, 768, 50])
text data projection =====> torch.Size([24, 30, 50])
text data transpose 1,2 =====> torch.Size([24, 50, 30])
audio data before transpose =====> torch.Size([24, 50, 5])
audio data after transpose =====> torch.Size([24, 5, 50])
audio data after projection =====> torch.Size([24, 30, 50])
audio data after transpose 1,2 =====> torch.Size([24, 50, 30])
end of audio data
torch.Size([24, 50, 30])
torch.Size([24, 30, 50])
end of text data
visual data before transpose ========> torch.Size([24, 50])
attention mask =====> torch.Size([24, 1, 1, 1])
attention mask squeeze =====> torch.Size([24, 1, 1])
attention mask permutation =====> torch.Size([24, 1, 1])
text data =====> torch.Size([24, 50, 768])
text data transpose =====> torch.Size([24, 768, 50])
text data projection =====> torch.Size([24, 30, 50])
text data transpose 1,2 =====> torch.Size([24, 50, 30])
audio data before transpose =====> torch.Size([24, 50, 5])
audio data after transpose =====> torch.Size([24, 5, 50])
audio data after projection =====> torch.Size([24, 30, 50])
audio data after transpose 1,2 =====> torch.Size([24, 50, 30])
end of audio data
torch.Size([24, 50, 30])
torch.Size([24, 30, 50])
end of text data
visual data before transpose ========> torch.Size([24, 50])
attention mask =====> torch.Size([24, 1, 1, 1])
attention mask squeeze =====> torch.Size([24, 1, 1])
attention mask permutation =====> torch.Size([24, 1, 1])
text data =====> torch.Size([24, 50, 768])
text data transpose =====> torch.Size([24, 768, 50])
text data projection =====> torch.Size([24, 30, 50])
text data transpose 1,2 =====> torch.Size([24, 50, 30])
audio data before transpose =====> torch.Size([24, 50, 5])
audio data after transpose =====> torch.Size([24, 5, 50])
audio data after projection =====> torch.Size([24, 30, 50])
audio data after transpose 1,2 =====> torch.Size([24, 50, 30])
end of audio data
torch.Size([24, 50, 30])
torch.Size([24, 30, 50])
end of text data
visual data before transpose ========> torch.Size([24, 50])
attention mask =====> torch.Size([24, 1, 1, 1])
attention mask squeeze =====> torch.Size([24, 1, 1])
attention mask permutation =====> torch.Size([24, 1, 1])
text data =====> torch.Size([24, 50, 768])
text data transpose =====> torch.Size([24, 768, 50])
text data projection =====> torch.Size([24, 30, 50])
text data transpose 1,2 =====> torch.Size([24, 50, 30])
audio data before transpose =====> torch.Size([24, 50, 5])
audio data after transpose =====> torch.Size([24, 5, 50])
audio data after projection =====> torch.Size([24, 30, 50])
audio data after transpose 1,2 =====> torch.Size([24, 50, 30])
end of audio data
torch.Size([24, 50, 30])
torch.Size([24, 30, 50])
end of text data
visual data before transpose ========> torch.Size([24, 50])
attention mask =====> torch.Size([24, 1, 1, 1])
attention mask squeeze =====> torch.Size([24, 1, 1])
attention mask permutation =====> torch.Size([24, 1, 1])
text data =====> torch.Size([24, 50, 768])
text data transpose =====> torch.Size([24, 768, 50])
text data projection =====> torch.Size([24, 30, 50])
text data transpose 1,2 =====> torch.Size([24, 50, 30])
audio data before transpose =====> torch.Size([24, 50, 5])
audio data after transpose =====> torch.Size([24, 5, 50])
audio data after projection =====> torch.Size([24, 30, 50])
audio data after transpose 1,2 =====> torch.Size([24, 50, 30])
end of audio data
torch.Size([24, 50, 30])
torch.Size([24, 30, 50])
end of text data
visual data before transpose ========> torch.Size([24, 50])
attention mask =====> torch.Size([24, 1, 1, 1])
attention mask squeeze =====> torch.Size([24, 1, 1])
attention mask permutation =====> torch.Size([24, 1, 1])
text data =====> torch.Size([24, 50, 768])
text data transpose =====> torch.Size([24, 768, 50])
text data projection =====> torch.Size([24, 30, 50])
text data transpose 1,2 =====> torch.Size([24, 50, 30])
audio data before transpose =====> torch.Size([24, 50, 5])
audio data after transpose =====> torch.Size([24, 5, 50])
audio data after projection =====> torch.Size([24, 30, 50])
audio data after transpose 1,2 =====> torch.Size([24, 50, 30])
end of audio data
torch.Size([24, 50, 30])
torch.Size([24, 30, 50])
end of text data
visual data before transpose ========> torch.Size([24, 50])
attention mask =====> torch.Size([24, 1, 1, 1])
attention mask squeeze =====> torch.Size([24, 1, 1])
attention mask permutation =====> torch.Size([24, 1, 1])
text data =====> torch.Size([24, 50, 768])
text data transpose =====> torch.Size([24, 768, 50])
text data projection =====> torch.Size([24, 30, 50])
text data transpose 1,2 =====> torch.Size([24, 50, 30])
audio data before transpose =====> torch.Size([24, 50, 5])
audio data after transpose =====> torch.Size([24, 5, 50])
audio data after projection =====> torch.Size([24, 30, 50])
audio data after transpose 1,2 =====> torch.Size([24, 50, 30])
end of audio data
torch.Size([24, 50, 30])
torch.Size([24, 30, 50])
end of text data
visual data before transpose ========> torch.Size([24, 50])
attention mask =====> torch.Size([24, 1, 1, 1])
attention mask squeeze =====> torch.Size([24, 1, 1])
attention mask permutation =====> torch.Size([24, 1, 1])
text data =====> torch.Size([24, 50, 768])
text data transpose =====> torch.Size([24, 768, 50])
text data projection =====> torch.Size([24, 30, 50])
text data transpose 1,2 =====> torch.Size([24, 50, 30])
audio data before transpose =====> torch.Size([24, 50, 5])
audio data after transpose =====> torch.Size([24, 5, 50])
audio data after projection =====> torch.Size([24, 30, 50])
audio data after transpose 1,2 =====> torch.Size([24, 50, 30])
end of audio data
torch.Size([24, 50, 30])
torch.Size([24, 30, 50])
end of text data
visual data before transpose ========> torch.Size([24, 50])
attention mask =====> torch.Size([24, 1, 1, 50])
attention mask squeeze =====> torch.Size([24, 1, 50])
attention mask permutation =====> torch.Size([24, 1, 50])
text data =====> torch.Size([24, 50, 768])
text data transpose =====> torch.Size([24, 768, 50])
text data projection =====> torch.Size([24, 30, 50])
text data transpose 1,2 =====> torch.Size([24, 50, 30])
audio data before transpose =====> torch.Size([24, 50])
attention mask =====> torch.Size([24, 1, 1, 50])
attention mask squeeze =====> torch.Size([24, 1, 50])
attention mask permutation =====> torch.Size([24, 1, 50])
text data =====> torch.Size([24, 50, 768])
text data transpose =====> torch.Size([24, 768, 50])
text data projection =====> torch.Size([24, 30, 50])
text data transpose 1,2 =====> torch.Size([24, 50, 30])
audio data before transpose =====> torch.Size([24, 50])
